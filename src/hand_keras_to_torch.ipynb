{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/root/miniconda3/envs/k2t_new/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''手动读取.h5文件赋值给torch'''\n",
    "import keras\n",
    "from models.DFCAN16 import DFCAN as keras_DFCAN\n",
    "from model.DFCAN import DFCAN as torch_DFCAN\n",
    "input_height, input_width, n_channel = 502, 502, 1\n",
    "\n",
    "weight_file = \"model_weights.h5\"\n",
    "\n",
    "keras_model = keras_DFCAN((input_height, input_width, n_channel))\n",
    "keras_model.load_weights(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载keras中可以学习的层的权重\n",
    "keras_layers_name = []\n",
    "keras_layers_weight = []\n",
    "for layer in keras_model.layers:\n",
    "    if layer.trainable == True:\n",
    "        keras_layers_name.append(layer.name)\n",
    "        keras_layers_weight.append(layer.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载pytorch权重\n",
    "torch_layers_names = []\n",
    "torch_layers_params = []\n",
    "model =  torch_DFCAN((n_channel))\n",
    "for name, param in model.named_parameters():\n",
    "    torch_layers_names.append(name)\n",
    "    if param.requires_grad:\n",
    "        # print(name, param.data)\n",
    "        torch_layers_params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有层的权重\n",
    "for indx, weight in enumerate(keras_model.get_weights()):\n",
    "    # weight = keras_model.get_weights()[0]   #keras第一层权重：numpy.ndarray\n",
    "    if len(weight.shape) == 4:  #卷积核\n",
    "        to_torch_weight = torch.Tensor(weight).permute(3,2,0,1) \n",
    "    elif len(weight.shape) == 1:  #偏置\n",
    "        to_torch_weight = torch.Tensor(weight)\n",
    "    else:\n",
    "        raise BaseException\n",
    "    # to_torch_weight = torch.Tensor(weight).permute(3,2,1,0)   \n",
    "    [k for k in model.named_parameters()][indx][1].data = to_torch_weight\n",
    "    # print(to_torch_weight[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for indx, weight in enumerate(keras_model.get_weights()):\n",
    "#     print(weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01893339 -0.06204253 -0.02285613]\n",
      " [ 0.02468616  0.02675407 -0.00809869]\n",
      " [-0.02100416 -0.08708128 -0.04743621]]\n",
      "tensor([[ 0.0189, -0.0620, -0.0229],\n",
      "        [ 0.0247,  0.0268, -0.0081],\n",
      "        [-0.0210, -0.0871, -0.0474]])\n"
     ]
    }
   ],
   "source": [
    "#查看权重\n",
    "indx = 12\n",
    "print(keras_model.get_weights()[indx][:,:,0,0])    #keras卷积核\n",
    "print([k for k in model.named_parameters()][indx][1][0,0,:,:].data)    #torch卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成固定输入\n",
    "torch.manual_seed(123)\n",
    "x0 = torch.randn(1, 1, 502, 502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #检查整个模型输出是否一致\n",
    "# #pytorch\n",
    "# torch_out = model(x0)\n",
    "\n",
    "# #keras\n",
    "# from keras import backend as K\n",
    "# keras_x0 = x0.permute(0,2,3,1).numpy()\n",
    "# print(type(keras_x0))\n",
    "# print(keras_x0.shape)\n",
    "# keras_output = keras_model.predict(keras_x0)\n",
    "# print(type(keras_output))\n",
    "# keras_output_torch_tensor = torch.Tensor(keras_output).permute(0, 3, 1, 2)\n",
    "\n",
    "# #比较二者\n",
    "# assert torch_out.shape == keras_output_torch_tensor.shape\n",
    "# print(torch_out == keras_output_torch_tensor)\n",
    "# print(torch_out.shape, keras_output_torch_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #查看一下卷积层输出\n",
    "# print(torch_out[0,0,:3,:3])\n",
    "# print(keras_output_torch_tensor[0,0,:3,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01893339 -0.06204253 -0.02285613]\n",
      " [ 0.02468616  0.02675407 -0.00809869]\n",
      " [-0.02100416 -0.08708128 -0.04743621]]\n",
      "tensor([[ 0.0189, -0.0620, -0.0229],\n",
      "        [ 0.0247,  0.0268, -0.0081],\n",
      "        [-0.0210, -0.0871, -0.0474]])\n",
      "0.0189334\n",
      "0.0189334\n"
     ]
    }
   ],
   "source": [
    "#查看数据类型是否一致\n",
    "# keras_model.get_weights()[indx][:,:,0,0].dtype\n",
    "# x0.dtype\n",
    "# [k for k in model.named_parameters()][indx][1][0,0,:,:].data.dtype\n",
    "print(keras_model.get_weights()[indx][:,:,0,0], [k for k in model.named_parameters()][indx][1][0,0,:,:].data,sep=\"\\n\")\n",
    "print(keras_model.get_weights()[indx][0,0,0,0], [k for k in model.named_parameters()][indx][1].detach().numpy()[0,0,0,0],sep=\"\\n\")\n",
    "#一致，都是float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #查看原始数据有没有出错\n",
    "# print(x0[0,0,:3,:3])\n",
    "# print(keras_x0[0,:3,:3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用官方的predict.py再跑跑看\n",
    "#官方的predict.py和本文都用的predict，但是predict函数里面batchsize=32，也就是很可能不是直接取的模型输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #检查卷积层输出是否一致\n",
    "# #pytorch\n",
    "# torch_out = model.input[0](x0)\n",
    "\n",
    "# #keras\n",
    "# from keras import backend as K\n",
    "# # keras_x0 = K.constant(x0.permute(0,2,3,1).numpy())  #转到keras的输入\n",
    "# keras_x0 = x0.permute(0,2,3,1).numpy()\n",
    "# # print(type(keras_x0))\n",
    "# # print(keras_x0.shape)\n",
    "# from keras.models import Model\n",
    "# try:\n",
    "#     layer_name = 'conv2d_84'\n",
    "#     keras_model.get_layer(layer_name)\n",
    "# except(ValueError):\n",
    "#     layer_name = 'conv2d_1'\n",
    "# intermediate_layer_model = Model(inputs=keras_model.input, outputs=keras_model.get_layer(layer_name).output)\n",
    "# keras_output = intermediate_layer_model.predict(keras_x0)\n",
    "# # print(\"keras卷积层输出：\",type(keras_output))\n",
    "# keras_output_torch_tensor = torch.Tensor(keras_output).permute(0, 3, 1, 2)\n",
    "\n",
    "# #比较二者\n",
    "# assert torch_out.shape == keras_output_torch_tensor.shape\n",
    "# # print(torch_out == keras_output_torch_tensor)\n",
    "# print(\"torch模型和keras模型卷积层输出shape：\",torch_out.shape, keras_output_torch_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用于pytorch打印的hook函数\n",
    "torch_inter_layer_out = None\n",
    "def get_layer_output(layer, input, output):\n",
    "    global torch_inter_layer_out\n",
    "    torch_inter_layer_out = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #检查第二层输出是否一致\n",
    "\n",
    "# #pytorch\n",
    "# model.input.register_forward_hook(get_layer_output) #添加钩子\n",
    "# model(x0)\n",
    "# torch_out = torch_inter_layer_out\n",
    "\n",
    "# #keras\n",
    "# from keras import backend as K\n",
    "# # keras_x0 = K.constant(x0.permute(0,2,3,1).numpy())  #转到keras的输入\n",
    "# keras_x0 = x0.permute(0,2,3,1).numpy()\n",
    "# # print(type(keras_x0))\n",
    "# # print(keras_x0.shape)\n",
    "# from keras.models import Model\n",
    "# try:\n",
    "#     layer_name = 'lambda_84'\n",
    "#     keras_model.get_layer(layer_name)\n",
    "# except(ValueError):\n",
    "#     layer_name = 'lambda_1'\n",
    "# intermediate_layer_model = Model(inputs=keras_model.input, outputs=keras_model.get_layer(layer_name).output)\n",
    "# keras_output = intermediate_layer_model.predict(keras_x0)\n",
    "# # print(\"keras卷积层输出：\",type(keras_output))\n",
    "# keras_output_torch_tensor = torch.Tensor(keras_output).permute(0, 3, 1, 2)\n",
    "\n",
    "# #比较二者\n",
    "# # assert torch_out.shape == keras_output_torch_tensor.shape\n",
    "# # print(torch_out == keras_output_torch_tensor)\n",
    "# print(\"torch模型和keras模型此层输出shape：\",torch_out.shape, keras_output_torch_tensor.shape)\n",
    "# print(\"torch模型和keras模型此层输出截取：\",torch_out[0,0,:3,:3], keras_output_torch_tensor[0,0,:3,:3],sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_layer_name(indx):\n",
    "    #获取keras模型第k层的layer_name\n",
    "    #获取keras_model所有层。实际上就是83个卷积层\n",
    "    trainable_layer_names = [layer.name.split(r\"/\")[0] for layer in keras_model.trainable_weights]\n",
    "    from collections import OrderedDict\n",
    "    ordered_dict = OrderedDict.fromkeys(trainable_layer_names)\n",
    "    set_trainable_layer_names = list(ordered_dict.keys())\n",
    "    # set_trainable_layer_names\n",
    "    # keras_model.get_layer(set_trainable_layer_names[0])\n",
    "    return set_trainable_layer_names[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGs[0].RCABs[0].conv_gelu1[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hook_layer\n",
    "pre_index = 1\n",
    "import re\n",
    "s = torch_layers_names[pre_index*2]\n",
    "s = re.sub(r'\\.(\\d+)\\.', r'[\\1].', s)\n",
    "s = re.sub(r'\\.(\\d+)$', r'[\\1]', s)\n",
    "s = re.sub(r'\\.[^.]*$', '', s)\n",
    "print(s)\n",
    "\n",
    "def get_member(cls, string):\n",
    "    # 将输入字符串按照`.`分割成列表\n",
    "    attribute_list = string.split(\".\")\n",
    "    \n",
    "    # 定义变量result，初始值为A\n",
    "    result = cls\n",
    "    \n",
    "    # 遍历列表，访问对应的成员\n",
    "    for attribute_name in attribute_list:\n",
    "        if \"[\" in attribute_name and \"]\" in attribute_name:\n",
    "            # 如果元素是以[数字]结尾的形式，就将数字提取出来，并使用getattr()函数访问result的对应成员\n",
    "            index = int(attribute_name[attribute_name.index(\"[\")+1:attribute_name.index(\"]\")])\n",
    "            attribute_name = attribute_name[:attribute_name.index(\"[\")]\n",
    "            result = getattr(result, attribute_name)[index]\n",
    "        else:\n",
    "            # 否则，直接使用getattr()函数访问result的对应成员\n",
    "            result = getattr(result, attribute_name)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 1, 3, 3], expected input[1, 64, 502, 502] to have 1 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a9fc8cd866f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhook_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_layer_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#添加钩子\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_inter_layer_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/k2t_new/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/DFCAN-pytorch/src/model/DFCAN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRGs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_gelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/k2t_new/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/k2t_new/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/k2t_new/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/k2t_new/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/k2t_new/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 1, 3, 3], expected input[1, 64, 502, 502] to have 1 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "# 检查第layer_index层\n",
    "layer_index = pre_index   #第3层\n",
    "hook_layer = get_member(model,s)\n",
    "hook_layer = model.RGs[0].RCABs[0].conv_gelu1[0]\n",
    "#pytorch\n",
    "hook_layer.register_forward_hook(get_layer_output) #添加钩子\n",
    "model(x0)\n",
    "torch_out = torch_inter_layer_out\n",
    "\n",
    "#keras\n",
    "from keras import backend as K\n",
    "# keras_x0 = K.constant(x0.permute(0,2,3,1).numpy())  #转到keras的输入\n",
    "keras_x0 = x0.permute(0,2,3,1).numpy()\n",
    "# print(type(keras_x0))\n",
    "# print(keras_x0.shape)\n",
    "from keras.models import Model\n",
    "layer_name = get_k_layer_name(layer_index)\n",
    "intermediate_layer_model = Model(inputs=keras_model.input, outputs=keras_model.get_layer(layer_name).output)\n",
    "keras_output = intermediate_layer_model.predict(keras_x0)\n",
    "# print(\"keras卷积层输出：\",type(keras_output))\n",
    "keras_output_torch_tensor = torch.Tensor(keras_output).permute(0, 3, 1, 2)\n",
    "\n",
    "#比较二者\n",
    "# assert torch_out.shape == keras_output_torch_tensor.shape\n",
    "# print(torch_out == keras_output_torch_tensor)\n",
    "print(\"torch模型和keras模型此层输出shape：\",torch_out.shape, keras_output_torch_tensor.shape)\n",
    "print(\"torch模型和keras模型此层输出截取：\",torch_out[0,0,:3,:3], keras_output_torch_tensor[0,0,:3,:3],sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #查看临时模型权重和torch_model权重\n",
    "# intermediate_layer_model.get_weights().__len__()\n",
    "# print(intermediate_layer_model.get_weights()[4][:, :, 0, 0])\n",
    "# print(torch_layers_params[4].data[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.modules at 0x7fb63445d780>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keras_model.layers)\n",
    "list(model.modules()).__len__()\n",
    "model.modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'se'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-83aece74a095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mnew_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mnew_layer0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlayer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mnew_layer0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m# new_layer1 = Dense(**layer_config[0])(new_layer0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# new_layer1.set_weights(layer_weights[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'se'"
     ]
    }
   ],
   "source": [
    "#单独看第三层\n",
    "# layer_index = pre_index   #第3层\n",
    "\n",
    "#构造输入\n",
    "torch.manual_seed(123)\n",
    "x0 = torch.randn((1, 64, 502, 502))\n",
    "\n",
    "#pytorch\n",
    "torch_out = model.RGs[0].RCABs[0].conv_gelu2[0](x0)\n",
    "\n",
    "### keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Model\n",
    "\n",
    "keras_x0 = x0.permute(0,2,3,1).numpy()\n",
    "##单独第三层拿出来构建模型\n",
    "#获取第3层\n",
    "layers = keras_model.layers[5:6] #第3个卷积层\n",
    "layer_config = [k.get_config() for k in layers]\n",
    "layer_weights = [k.get_weights() for k in layers]\n",
    "#构建单层模型\n",
    "new_input = Input(shape=layer.input_shape[1:])\n",
    "new_layer0 = Conv2D(**layer_config[0])(new_input)\n",
    "new_layer0.se\n",
    "# new_layer1 = Dense(**layer_config[0])(new_layer0)\n",
    "# new_layer1.set_weights(layer_weights[0])\n",
    "new_model = Model(new_input, new_layer0)\n",
    "#拿来做预测\n",
    "keras_output = new_model.predict(keras_x0)\n",
    "# print(\"keras卷积层输出：\",type(keras_output))\n",
    "keras_output_torch_tensor = torch.Tensor(keras_output).permute(0, 3, 1, 2)\n",
    "\n",
    "#比较二者\n",
    "# assert torch_out.shape == keras_output_torch_tensor.shape\n",
    "# print(torch_out == keras_output_torch_tensor)\n",
    "print(\"torch模型和keras模型此层输出shape：\",torch_out.shape, keras_output_torch_tensor.shape)\n",
    "print(\"torch模型和keras模型此层输出截取：\",torch_out[0,0,:3,:3], keras_output_torch_tensor[0,0,:3,:3],sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##可能是Torch和DDCAN的gelu定义不同，看看第1个卷积的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = layers[0]\n",
    "new_layer = Conv2D.from_config(layer.get_config())\n",
    "new_layer.set_weights(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "import numpy as np\n",
    "\n",
    "# 假设我们有一个Conv2D层\n",
    "conv_layer = Conv2D(filters=32, kernel_size=(3,3), input_shape=(28,28,1))\n",
    "\n",
    "# 获取该层的配置和权重\n",
    "layer_config = conv_layer.get_config()\n",
    "layer_weights = conv_layer.get_weights()\n",
    "\n",
    "# 创建一个新的Conv2D层\n",
    "new_layer = Conv2D.from_config(layer_config)\n",
    "\n",
    "# 设置新层的权重\n",
    "new_layer.set_weights(layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(conv_layer)\n",
    "type(layers[0])\n",
    "layers[0].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_layers_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.layers[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keras_model.layers[5].get_weights()[0][:,:,0,0])\n",
    "keras_model.get_weights()[4][:,:,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch_layers_names[:10])\n",
    "print([get_k_layer_name(k) for k in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.RGs[0].RCABs[0].conv_gelu2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.children())\n",
    "print(list(model.named_parameters())[layer_index*2][1][0,0,:,:])\n",
    "print(keras_model.get_weights()[layer_index*2][:,:,0,0])\n",
    "\n",
    "# model._modules\n",
    "# # model.RGs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch_out[0,0,:3,:3], keras_output_torch_tensor[0,0,:3,:3],sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate_layer_model.summary()\n",
    "print(\"临时keras卷积核\\n\",intermediate_layer_model.get_weights()[0][:,:,0,0])\n",
    "print(\"临时keras卷积偏置\", intermediate_layer_model.get_weights()[1][:3])\n",
    "print(\"临时torch卷积核\\n\",[k for k in model.input[0].named_parameters()][0][1].data[0,0,:3,:3])\n",
    "print(\"临时torch卷积偏置\", torch_layers_params[1][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keras_model.get_weights()[0][:,:,0,0])\n",
    "print(torch_layers_params[0][0,0,:,:].data)\n",
    "torch_layers_params[0].shape\n",
    "# keras_model.get_weights()[0].shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d51c37cc946fd97d9f55e4af32d69fbde6fa060298c74a830a683b474425584"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 ('k2t_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
